{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Executing code: \n",
    "Python hclust.py iris.dat 3\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Change log: \n",
    "\n",
    "- Nov 8, 2015\n",
    "1. Change the logic to calculation centroid\n",
    "2. Add judgement for some invalid input cases\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import heapq\n",
    "import itertools\n",
    "\n",
    "class Hierarchical_Clustering:\n",
    "    def __init__(self, ipt_data, ipt_k):\n",
    "        self.input_file_name = ipt_data\n",
    "        self.k = ipt_k\n",
    "        self.dataset = None\n",
    "        self.dataset_size = 0\n",
    "        self.dimension = 0\n",
    "        self.heap = []\n",
    "        self.clusters = []\n",
    "        self.gold_standard = {}\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize and check parameters\n",
    "\n",
    "        \"\"\"\n",
    "        # check file exist and if it's a file or dir\n",
    "        if not os.path.isfile(self.input_file_name):\n",
    "            self.quit(\"Input file doesn't exist or it's not a file\")\n",
    "\n",
    "        self.dataset, self.clusters, self.gold_standard = self.load_data(self.input_file_name)\n",
    "        self.dataset_size = len(self.dataset)\n",
    "\n",
    "        if self.dataset_size == 0:\n",
    "            self.quit(\"Input file doesn't include any data\")\n",
    "\n",
    "        if self.k == 0:\n",
    "            self.quit(\"k = 0, no cluster will be generated\")\n",
    "\n",
    "        if self.k > self.dataset_size:\n",
    "            self.quit(\"k is larger than the number of existing clusters\")\n",
    "\n",
    "        self.dimension = len(self.dataset[0][\"data\"])\n",
    "\n",
    "        if self.dimension == 0:\n",
    "            self.quit(\"dimension for dataset cannot be zero\")\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"                      Hierarchical Clustering Functions                       \"\"\"\n",
    "    \"\"\"                                                                              \"\"\"    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    def euclidean_distance(self, data_point_one, data_point_two):\n",
    "        \"\"\"\n",
    "        euclidean distance: https://en.wikipedia.org/wiki/Euclidean_distance\n",
    "        assume that two data points have same dimension\n",
    "\n",
    "        \"\"\"\n",
    "        size = len(data_point_one)\n",
    "        result = 0.0\n",
    "        for i in range(size):\n",
    "            f1 = float(data_point_one[i])   # feature for data one\n",
    "            f2 = float(data_point_two[i])   # feature for data two\n",
    "            tmp = f1 - f2\n",
    "            result += pow(tmp, 2)\n",
    "        result = math.sqrt(result)\n",
    "        return result\n",
    "\n",
    "    def compute_pairwise_distance(self, dataset):\n",
    "        result = []\n",
    "        dataset_size = len(dataset)\n",
    "        for i in range(dataset_size-1):    # ignore last i\n",
    "            for j in range(i+1, dataset_size):     # ignore duplication\n",
    "                dist = self.euclidean_distance(dataset[i][\"data\"], dataset[j][\"data\"])\n",
    "\n",
    "                # duplicate dist, need to be remove, and there is no difference to use tuple only\n",
    "                # leave second dist here is to take up a position for tie selection\n",
    "                result.append( (dist, [dist, [[i], [j]]]) )\n",
    "\n",
    "        return result\n",
    "                \n",
    "    def build_priority_queue(self, distance_list):\n",
    "        heapq.heapify(distance_list)\n",
    "        self.heap = distance_list\n",
    "        return self.heap\n",
    "\n",
    "    def compute_centroid_two_clusters(self, current_clusters, data_points_index):\n",
    "        size = len(data_points_index)\n",
    "        dim = self.dimension\n",
    "        centroid = [0.0]*dim\n",
    "        for index in data_points_index:\n",
    "            dim_data = current_clusters[str(index)][\"centroid\"]\n",
    "            for i in range(dim):\n",
    "                centroid[i] += float(dim_data[i])\n",
    "        for i in range(dim):\n",
    "            centroid[i] /= size\n",
    "        return centroid\n",
    "\n",
    "    def compute_centroid(self, dataset, data_points_index):\n",
    "        size = len(data_points_index)\n",
    "        dim = self.dimension\n",
    "        centroid = [0.0]*dim\n",
    "        for idx in data_points_index:\n",
    "            dim_data = dataset[idx][\"data\"]\n",
    "            for i in range(dim):\n",
    "                centroid[i] += float(dim_data[i])\n",
    "        for i in range(dim):\n",
    "            centroid[i] /= size\n",
    "        return centroid\n",
    "\n",
    "    def hierarchical_clustering(self):\n",
    "        \"\"\"\n",
    "        Main Process for hierarchical clustering\n",
    "\n",
    "        \"\"\"\n",
    "        dataset = self.dataset\n",
    "        current_clusters = self.clusters\n",
    "        old_clusters = []\n",
    "        heap = hc.compute_pairwise_distance(dataset)\n",
    "        heap = hc.build_priority_queue(heap)\n",
    "\n",
    "        while len(current_clusters) > self.k:\n",
    "            dist, min_item = heapq.heappop(heap)\n",
    "            # pair_dist = min_item[0]\n",
    "            pair_data = min_item[1]\n",
    "\n",
    "            # judge if include old cluster\n",
    "            if not self.valid_heap_node(min_item, old_clusters):\n",
    "                continue\n",
    "\n",
    "            new_cluster = {}\n",
    "            new_cluster_elements = sum(pair_data, [])\n",
    "            new_cluster_cendroid = self.compute_centroid(dataset, new_cluster_elements)\n",
    "            new_cluster_elements.sort()\n",
    "            new_cluster.setdefault(\"centroid\", new_cluster_cendroid)\n",
    "            new_cluster.setdefault(\"elements\", new_cluster_elements)\n",
    "            for pair_item in pair_data:\n",
    "                old_clusters.append(pair_item)\n",
    "                del current_clusters[str(pair_item)]\n",
    "            self.add_heap_entry(heap, new_cluster, current_clusters)\n",
    "            current_clusters[str(new_cluster_elements)] = new_cluster\n",
    "        # current_clusters.sort()\n",
    "        sorted(current_clusters)\n",
    "        \n",
    "        return current_clusters\n",
    "            \n",
    "    def valid_heap_node(self, heap_node, old_clusters):\n",
    "        pair_dist = heap_node[0]\n",
    "        pair_data = heap_node[1]\n",
    "        for old_cluster in old_clusters:\n",
    "            if old_cluster in pair_data:\n",
    "                return False\n",
    "        return True\n",
    "            \n",
    "    def add_heap_entry(self, heap, new_cluster, current_clusters):\n",
    "        for ex_cluster in current_clusters.values():\n",
    "            new_heap_entry = []\n",
    "            dist = self.euclidean_distance(ex_cluster[\"centroid\"], new_cluster[\"centroid\"])\n",
    "            new_heap_entry.append(dist)\n",
    "            new_heap_entry.append([new_cluster[\"elements\"], ex_cluster[\"elements\"]])\n",
    "            heapq.heappush(heap, (dist, new_heap_entry))\n",
    "\n",
    "    def evaluate(self, current_clusters):\n",
    "        gold_standard = self.gold_standard\n",
    "        current_clustes_pairs = []\n",
    "\n",
    "        for (current_cluster_key, current_cluster_value) in current_clusters.items():\n",
    "            tmp = list(itertools.combinations(current_cluster_value[\"elements\"], 2))\n",
    "            current_clustes_pairs.extend(tmp)\n",
    "        tp_fp = len(current_clustes_pairs)\n",
    "\n",
    "        gold_standard_pairs = []\n",
    "        for (gold_standard_key, gold_standard_value) in gold_standard.items():\n",
    "            tmp = list(itertools.combinations(gold_standard_value, 2))\n",
    "            gold_standard_pairs.extend(tmp)\n",
    "        tp_fn = len(gold_standard_pairs)\n",
    "\n",
    "        tp = 0.0\n",
    "        for ccp in current_clustes_pairs:\n",
    "            if ccp in gold_standard_pairs:\n",
    "                tp += 1\n",
    "\n",
    "        if tp_fp == 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            precision = tp/tp_fp\n",
    "        if tp_fn == 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            recall = tp/tp_fn\n",
    "\n",
    "        return precision, recall\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \"\"\"                             Helper Functions                                 \"\"\"\n",
    "    \"\"\"                                                                              \"\"\"    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    def load_data(self, input_file_name):\n",
    "        \"\"\"\n",
    "        load data and do some preparations\n",
    "\n",
    "        \"\"\"\n",
    "        input_file = open(input_file_name, 'rU')\n",
    "        dataset = []\n",
    "        clusters = {}\n",
    "        gold_standard = {}\n",
    "        id = 0\n",
    "        for line in input_file:\n",
    "            line = line.strip('\\n')\n",
    "            row = str(line)\n",
    "            row = row.split(\",\")\n",
    "            iris_class = row[-1]\n",
    "\n",
    "            data = {}\n",
    "            data.setdefault(\"id\", id)   # duplicate\n",
    "            data.setdefault(\"data\", row[:-1])\n",
    "            data.setdefault(\"class\", row[-1])\n",
    "            dataset.append(data)\n",
    "\n",
    "            clusters_key = str([id])\n",
    "            clusters.setdefault(clusters_key, {})\n",
    "            clusters[clusters_key].setdefault(\"centroid\", row[:-1])\n",
    "            clusters[clusters_key].setdefault(\"elements\", [id])\n",
    "\n",
    "            gold_standard.setdefault(iris_class, [])\n",
    "            gold_standard[iris_class].append(id)\n",
    "\n",
    "            id += 1\n",
    "        return dataset, clusters, gold_standard\n",
    "\n",
    "    def quit(self, err_desc):\n",
    "        raise SystemExit('\\n'+ \"PROGRAM EXIT: \" + err_desc + ', please check your input' + '\\n')\n",
    "\n",
    "    def loaded_dataset(self):\n",
    "        \"\"\"\n",
    "        use for test only\n",
    "\n",
    "        \"\"\"\n",
    "        return self.dataset\n",
    "\n",
    "    def display(self, current_clusters, precision, recall):\n",
    "        print precision\n",
    "        print recall\n",
    "        clusters = current_clusters.values()\n",
    "        for cluster in clusters:\n",
    "            cluster[\"elements\"].sort()\n",
    "            print cluster[\"elements\"]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\"                               Main Method                                    \"\"\"\n",
    "\"\"\"                                                                              \"\"\"    \n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - ipt_data: a text file name for the input data\n",
    "    - ipt_k: a value k for the number of desired clusters.\n",
    "\n",
    "    outputs:\n",
    "    - opt_clusters: output k clusters, with each cluster contains a set of data points (index for input data)\n",
    "    - opt_precision\n",
    "    - opt_recall\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ## input test\n",
    "    # ipt_data = \"iris.dat\"\n",
    "    # ipt_data = \"iris_dataset1.txt\"\n",
    "    # ipt_k = 3\n",
    "\n",
    "    ipt_data = sys.argv[1]      # input data, e.g. iris.dat\n",
    "    ipt_k = int(sys.argv[2])    # number of clusters, e.g. 3\n",
    "\n",
    "    hc = Hierarchical_Clustering(ipt_data, ipt_k)\n",
    "    hc.initialize()\n",
    "    current_clusters = hc.hierarchical_clustering()\n",
    "    precision, recall = hc.evaluate(current_clusters)\n",
    "    hc.display(current_clusters, precision, recall)\n",
    "\n",
    "    ## euclidean_distance() test\n",
    "    # loaded_data = hc.loaded_dataset()\n",
    "    # print loaded_data\n",
    "    # print hc.euclidean_distance(loaded_data[0][\"data\"],loaded_data[1][\"data\"])\n",
    "\n",
    "    ## compute_centroid() test\n",
    "    # loaded_data = hc.loaded_dataset()\n",
    "    # hc.compute_centroid(loaded_data, [10, 11, 12, 13])\n",
    "\n",
    "    ## distance_list test\n",
    "    # distance_list = hc.compute_pairwise_distance()\n",
    "    # distance_list.sort()\n",
    "    # print distance_list\n",
    "    \n",
    "    ## heapq test\n",
    "    # heap = []\n",
    "    # data = [1, 3, 5, 7, 9, 2, 4, 6, 8, 0]\n",
    "    # data = [[1,4,5], [3,6,1], [5,6,10], [7,2,11], [9,6,1], [2,1,5], [4,2,1], [6,6,5], [8,7,1], [0,1,0]]\n",
    "    # heapq.heapify(data)\n",
    "    # print data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
